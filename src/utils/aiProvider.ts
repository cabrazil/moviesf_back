import axios from 'axios';
import { GoogleGenerativeAI } from '@google/generative-ai';

export type AIProvider = 'openai' | 'gemini' | 'deepseek' | 'kimi';

// Interfaces para tipagem das respostas das APIs
interface OpenAIResponse {
  choices: Array<{
    message: {
      content: string;
    };
  }>;
}

interface GeminiResponse {
  candidates: Array<{
    content: {
      parts: Array<{
        text: string;
      }>;
    };
  }>;
}

interface DeepSeekResponse {
  choices: Array<{
    message: {
      content: string;
    };
  }>;
}

interface KimiResponse {
  choices: Array<{
    message: {
      content: string;
    };
  }>;
}

export interface AIResponse {
  content: string;
  success: boolean;
  error?: string;
}

export interface AIConfig {
  provider: AIProvider;
  model?: string;
  temperature?: number;
  maxTokens?: number;
}

class AIProviderManager {
  private config: AIConfig;

  constructor(config: AIConfig) {
    this.config = config;
  }

  async generateResponse(
    systemPrompt: string,
    userPrompt: string,
    options?: {
      temperature?: number;
      maxTokens?: number;
    }
  ): Promise<AIResponse> {
    const temperature = options?.temperature ?? this.config.temperature ?? 0.7;
    const maxTokens = options?.maxTokens ?? this.config.maxTokens ?? 600;

    switch (this.config.provider) {
      case 'openai':
        return this.generateOpenAIResponse(systemPrompt, userPrompt, temperature, maxTokens);
      case 'gemini':
        return this.generateGeminiResponse(systemPrompt, userPrompt, temperature, maxTokens);
      case 'deepseek':
        return this.generateDeepSeekResponse(systemPrompt, userPrompt, temperature, maxTokens);
      case 'kimi':
        return this.generateKimiResponse(systemPrompt, userPrompt, temperature, maxTokens);
      default:
        throw new Error(`Provedor de IA n√£o suportado: ${this.config.provider}`);
    }
  }

  private async generateOpenAIResponse(
    systemPrompt: string,
    userPrompt: string,
    temperature: number,
    maxTokens: number
  ): Promise<AIResponse> {
    try {
      const modelToUse = this.config.model || 'gpt-4-turbo';

      const response = await axios.post<OpenAIResponse>('https://api.openai.com/v1/chat/completions', {
        model: modelToUse,
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: userPrompt }
        ],
        temperature,
        max_tokens: maxTokens
      }, {
        headers: {
          'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
          'Content-Type': 'application/json'
        }
      });

      const content = response.data.choices[0].message.content;
      return { content, success: true };
    } catch (error: any) {
      const status = error?.response?.status;
      const errorMessage = error?.response?.data?.error?.message || error?.message;
      const modelUsed = this.config.model || 'gpt-4-turbo';

      console.error(`Erro na API OpenAI (modelo: ${modelUsed}):`, {
        status,
        message: errorMessage,
        error: error?.response?.data
      });

      return {
        content: '',
        success: false,
        error: `Erro OpenAI (${status || 'N/A'}): ${errorMessage || 'Erro desconhecido'}`
      };
    }
  }

  private async generateContentWithOpenAI(
    userPrompt: string,
    systemPrompt: string
  ): Promise<AIResponse> {
    try {
      const response = await axios.post<OpenAIResponse>('https://api.openai.com/v1/chat/completions', {
        model: 'gpt-4-turbo',
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: userPrompt }
        ],
        temperature: 0.2,
        max_tokens: 1500
      }, {
        headers: {
          'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
          'Content-Type': 'application/json'
        }
      });

      const content = response.data.choices[0].message.content;
      return { content, success: true };
    } catch (error) {
      console.error('Erro no fallback OpenAI:', error);
      return {
        content: '',
        success: false,
        error: `Erro OpenAI Fallback: ${error instanceof Error ? error.message : String(error)}`
      };
    }
  }

  /**
   * Extrai JSON de uma string que pode conter markdown ou texto adicional.
   * Se n√£o encontrar JSON v√°lido, retorna o texto original (√∫til para texto puro como hooks e warnings).
   */
  private extractJSONFromResponse(text: string): string {
    // Tentar encontrar JSON dentro de markdown code blocks
    const markdownJsonMatch = text.match(/```(?:json)?\s*(\{[\s\S]*?\})\s*```/);
    if (markdownJsonMatch) {
      try {
        // Validar se √© JSON v√°lido
        JSON.parse(markdownJsonMatch[1]);
        return markdownJsonMatch[1];
      } catch {
        // Se n√£o for JSON v√°lido, continuar procurando
      }
    }

    // Tentar encontrar JSON direto (pode estar no in√≠cio ou meio do texto)
    const jsonMatch = text.match(/\{[\s\S]*\}/);
    if (jsonMatch) {
      try {
        // Validar se √© JSON v√°lido
        JSON.parse(jsonMatch[0]);
        return jsonMatch[0];
      } catch {
        // Se n√£o for JSON v√°lido, pode ser texto que cont√©m chaves mas n√£o √© JSON
        // Retornar texto original para preservar conte√∫do de hooks/warnings
      }
    }

    // Se n√£o encontrou JSON v√°lido, retornar o texto original
    // Isso √© importante para hooks e warnings que s√£o texto puro
    return text;
  }

  private async generateGeminiResponse(
    systemPrompt: string,
    userPrompt: string,
    temperature: number,
    maxTokens: number
  ): Promise<AIResponse> {
    // Verificar se esperamos JSON (an√°lise de sentimentos) ou texto puro (hooks, warnings)
    const expectsJSON = systemPrompt.includes('suggestedSubSentiments') ||
      systemPrompt.includes('JSON') ||
      userPrompt.includes('suggestedSubSentiments') ||
      userPrompt.includes('JSON v√°lido');

    let enhancedSystemPrompt: string;

    if (expectsJSON) {
      // Prompt otimizado para an√°lise de sentimentos (JSON obrigat√≥rio)
      enhancedSystemPrompt = `
${systemPrompt}

INSTRU√á√ïES ESPEC√çFICAS PARA AN√ÅLISE PRECISA:
1. PRIORIZE SEMPRE subsentimentos que j√° existem na base de dados
2. Para cada g√™nero, considere os padr√µes comprovados:
   - A√ß√£o/Aventura: "Adrenalina / Emo√ß√£o Intensa", "Deslumbramento Visual", "Inspira√ß√£o / Motiva√ß√£o para Agir"
   - Romance/Com√©dia Rom√¢ntica: "Conforto / Aconchego Emocional", "Do√ßura / Encanto", "Nostalgia (Positiva)"
   - Fam√≠lia/Anima√ß√£o: "Leveza / Divers√£o Descompromissada", "Conforto / Aconchego Emocional", "Do√ßura / Encanto"
   - Suspense/Thriller: "Suspense Crescente", "Desintegra√ß√£o Psicol√≥gica", "Desespero Crescente"
   - Drama Guerra: "Conflito e Sobreviv√™ncia", "Esperan√ßa e Supera√ß√£o", "Conex√£o Humana e Natureza"
   - Coming-of-age: "Autodescoberta e Crescimento", "Esperan√ßa e Supera√ß√£o", "Conex√£o Humana e Natureza"
3. EVITE criar novos subsentimentos sem necessidade cr√≠tica real
4. Use vocabul√°rio t√©cnico cinematogr√°fico preciso e espec√≠fico
5. Foque na ESS√äNCIA emocional do filme para a jornada do usu√°rio

FORMATO DE RESPOSTA OBRIGAT√ìRIO - CR√çTICO:
‚ö†Ô∏è IMPORTANTE: Responda APENAS com JSON v√°lido, SEM markdown, SEM texto adicional, SEM explica√ß√µes.
‚ö†Ô∏è N√ÉO use blocos de c√≥digo markdown (tr√™s backticks seguidos).
‚ö†Ô∏è N√ÉO adicione texto antes ou depois do JSON.
‚ö†Ô∏è Responda DIRETAMENTE com o JSON puro no formato exato abaixo:

{
  "suggestedSubSentiments": [
    {
      "name": "Nome do SubSentimento",
      "relevance": 0.95,
      "explanation": "Explica√ß√£o detalhada",
      "isNew": false
    }
  ]
}
`;
    } else {
      // Prompt para texto puro (hooks, warnings, etc.) - sem instru√ß√µes r√≠gidas de JSON
      enhancedSystemPrompt = `
${systemPrompt}

INSTRU√á√ïES IMPORTANTES:
‚ö†Ô∏è Responda APENAS com o texto solicitado, SEM markdown, SEM blocos de c√≥digo, SEM JSON.
‚ö†Ô∏è N√ÉO use formata√ß√£o markdown (tr√™s backticks seguidos).
‚ö†Ô∏è N√ÉO adicione explica√ß√µes ou texto adicional.
‚ö†Ô∏è Responda DIRETAMENTE com o conte√∫do solicitado.
`;
    }

    // Combinar prompts otimizados
    const combinedPrompt = `${enhancedSystemPrompt}\n\n${userPrompt}`;

    const modelToUse = this.config.model || 'gemini-2.5-flash';

    console.log(`ü§ñ Tentando Gemini com biblioteca oficial (modelo: ${modelToUse})...`);

    // Verificar se a chave de API est√° dispon√≠vel
    if (!process.env.GEMINI_API_KEY) {
      console.error('‚ùå GEMINI_API_KEY n√£o encontrada nas vari√°veis de ambiente');
      return {
        content: '',
        success: false,
        error: 'GEMINI_API_KEY n√£o configurada'
      };
    }

    try {
      // Inicializar a biblioteca oficial do Google
      const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

      // Obter o modelo
      const model = genAI.getGenerativeModel({
        model: modelToUse,
        generationConfig: {
          temperature: 0.2,           // Mais determin√≠stico
          maxOutputTokens: maxTokens, // Usar o parametro passado (default 2000 ou o que vier do script)
          topP: 0.8,
          topK: 20
        }
      });

      // Log apenas em desenvolvimento para debug detalhado
      if (process.env.NODE_ENV === 'development') {
        console.log(`üîç Usando biblioteca oficial @google/generative-ai`);
        console.log(`üìè Tamanho do prompt: ${combinedPrompt.length} caracteres`);
        console.log(`üé´ Max Tokens solicitados: ${maxTokens}`);
      }

      // Gerar conte√∫do usando a biblioteca oficial
      const result = await model.generateContent(combinedPrompt);
      const response = await result.response;

      if (!response || !response.text) {
        throw new Error('Resposta vazia - nenhum texto retornado');
      }

      let content = response.text();

      // Extrair JSON se vier em markdown ou com texto adicional
      content = this.extractJSONFromResponse(content);

      return {
        content,
        success: true
      };
    } catch (error: any) {
      const errorMessage = error?.message || 'Erro desconhecido';
      const status = error?.status || error?.response?.status;

      // Log do erro
      if (status === 429 || errorMessage.includes('429') || errorMessage.includes('quota')) {
        console.error(`Erro 429 (Quota excedida) na API Gemini`);
      } else if (status === 503 || errorMessage.includes('503') || errorMessage.includes('unavailable')) {
        console.error(`Erro 503 (Service Unavailable) na API Gemini`);
      } else if (status === 404 || errorMessage.includes('404') || errorMessage.includes('not found')) {
        console.error(`Erro 404 (Modelo n√£o encontrado): ${modelToUse}`);
      } else {
        console.error(`Erro na API Gemini:`, errorMessage);
      }

      // Fallback 1: DeepSeek se dispon√≠vel (for√ßar modelo correto)
      if (process.env.DEEPSEEK_API_KEY) {
        try {
          console.log('üîÑ Erro persistente no Gemini - tentando fallback para DeepSeek (modelo: deepseek-chat)...');
          const deepseekResult = await this.generateDeepSeekResponse(systemPrompt, userPrompt, temperature, maxTokens, 'deepseek-chat');
          if (deepseekResult.success) {
            console.log('‚úÖ Fallback para DeepSeek bem-sucedido (modelo: deepseek-chat)');
            return deepseekResult;
          } else {
            console.warn('‚ö†Ô∏è Fallback DeepSeek retornou erro:', deepseekResult.error);
          }
        } catch (fallbackError: any) {
          console.error('‚ùå Fallback para DeepSeek falhou:', fallbackError?.message || fallbackError);
        }
      }

      // Fallback 2: OpenAI como √∫ltimo recurso
      if (process.env.OPENAI_API_KEY) {
        try {
          console.log('üîÑ DeepSeek falhou - tentando fallback final para OpenAI (modelo: gpt-4-turbo)...');
          const openaiResult = await this.generateOpenAIResponse(systemPrompt, userPrompt, temperature, maxTokens);
          if (openaiResult.success) {
            console.log('‚úÖ Fallback para OpenAI bem-sucedido (modelo: gpt-4-turbo)');
            return openaiResult;
          } else {
            console.warn('‚ö†Ô∏è Fallback OpenAI retornou erro:', openaiResult.error);
          }
        } catch (fallbackError: any) {
          console.error('‚ùå Fallback para OpenAI tamb√©m falhou:', fallbackError?.message || fallbackError);
        }
      }

      return {
        content: '',
        success: false,
        error: 'Erro Gemini: falha ap√≥s tentar biblioteca oficial, fallback DeepSeek e fallback OpenAI.'
      };
    }
  }

  private async generateDeepSeekResponse(
    systemPrompt: string,
    userPrompt: string,
    temperature: number,
    maxTokens: number,
    forcedModel?: string
  ): Promise<AIResponse> {
    try {
      // Usar modelo for√ßado se fornecido, sen√£o usar o do config, sen√£o usar padr√£o
      // Isso corrige o bug onde fallback do Gemini tentava usar 'gemini-2.5-flash' no DeepSeek
      const modelToUse = forcedModel || this.config.model || 'deepseek-chat';

      // Log do modelo sendo usado (apenas se for diferente do esperado)
      if (forcedModel && forcedModel !== this.config.model) {
        console.log(`üìå Usando modelo for√ßado para DeepSeek: ${modelToUse} (config original: ${this.config.model})`);
      }

      // Validar e limitar maxTokens (alguns modelos t√™m limites)
      const safeMaxTokens = Math.min(maxTokens, 4000); // Limite seguro para DeepSeek

      // Validar tamanho do prompt (alguns modelos t√™m limite de contexto)
      const combinedPrompt = `${systemPrompt}\n\n${userPrompt}`;
      if (combinedPrompt.length > 100000) {
        console.warn('‚ö†Ô∏è Prompt muito longo para DeepSeek, truncando...');
        const truncatedUserPrompt = userPrompt.substring(0, 50000);
        const response = await axios.post<DeepSeekResponse>('https://api.deepseek.com/v1/chat/completions', {
          model: modelToUse,
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: truncatedUserPrompt }
          ],
          temperature,
          max_tokens: safeMaxTokens
        }, {
          headers: {
            'Authorization': `Bearer ${process.env.DEEPSEEK_API_KEY}`,
            'Content-Type': 'application/json'
          }
        });

        const content = response.data.choices[0].message.content;
        return { content, success: true };
      }

      const response = await axios.post<DeepSeekResponse>('https://api.deepseek.com/v1/chat/completions', {
        model: modelToUse,
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: userPrompt }
        ],
        temperature,
        max_tokens: safeMaxTokens
      }, {
        headers: {
          'Authorization': `Bearer ${process.env.DEEPSEEK_API_KEY}`,
          'Content-Type': 'application/json'
        }
      });

      if (!response.data.choices || response.data.choices.length === 0) {
        throw new Error('Resposta vazia - nenhuma escolha retornada');
      }

      const content = response.data.choices[0].message.content;
      return { content, success: true };
    } catch (error: any) {
      const status = error?.response?.status;
      const errorMessage = error?.response?.data?.error?.message || error?.response?.data?.message || error?.message;

      const modelUsed = forcedModel || this.config.model || 'deepseek-chat';
      console.error(`Erro na API DeepSeek (modelo: ${modelUsed}):`, {
        status,
        message: errorMessage,
        error: error?.response?.data
      });

      return {
        content: '',
        success: false,
        error: `Erro DeepSeek (${status || 'N/A'}): ${errorMessage || 'Erro desconhecido'}`
      };
    }
  }

  private async generateKimiResponse(
    systemPrompt: string,
    userPrompt: string,
    temperature: number,
    maxTokens: number
  ): Promise<AIResponse> {
    try {
      const modelToUse = this.config.model || 'moonshotai/kimi-k2.5';
      const invokeUrl = "https://integrate.api.nvidia.com/v1/chat/completions";

      console.log(`ü§ñ Tentando Kimi (via Nvidia) modelo: ${modelToUse}...`);

      if (!process.env.KIMI_API_KEY) {
        throw new Error('KIMI_API_KEY n√£o configurada nas vari√°veis de ambiente');
      }

      const response = await axios.post<KimiResponse>(invokeUrl, {
        model: modelToUse,
        messages: [
          { role: 'system', content: systemPrompt },
          { role: "user", content: userPrompt }
        ],
        max_tokens: maxTokens, // User example limits to 16384, but we use strict args
        temperature: temperature,
        top_p: 1.00,
        stream: false,
        chat_template_kwargs: { thinking: true } // As per user example
      }, {
        headers: {
          "Authorization": `Bearer ${process.env.KIMI_API_KEY}`,
          "Accept": "application/json",
          "Content-Type": "application/json"
        }
      });

      if (!response.data.choices || response.data.choices.length === 0) {
        throw new Error('Resposta vazia - nenhuma escolha retornada');
      }

      const content = response.data.choices[0].message.content;
      return { content, success: true };

    } catch (error: any) {
      const status = error?.response?.status;
      const errorMessage = error?.response?.data?.error?.message || error?.response?.data?.message || error?.message;

      console.error(`Erro na API Kimi/Nvidia:`, {
        status,
        message: errorMessage,
        error: error?.response?.data
      });

      return {
        content: '',
        success: false,
        error: `Erro Kimi (${status || 'N/A'}): ${errorMessage || 'Erro desconhecido'}`
      };
    }
  }
}

export function createAIProvider(config: AIConfig): AIProviderManager {
  return new AIProviderManager(config);
}

export function getDefaultConfig(provider: AIProvider): AIConfig {
  const modelMap = {
    openai: 'gpt-3.5-turbo', // Downgrade seguro para evitar erros de cota (era gpt-4-turbo)
    gemini: 'gemini-2.5-flash',
    deepseek: 'deepseek-chat',
    kimi: 'moonshotai/kimi-k2.5'
  };

  return {
    provider,
    model: modelMap[provider],
    temperature: provider === 'deepseek' || provider === 'kimi' ? 1.0 : 0.7,
    maxTokens: provider === 'kimi' ? 4000 : 2000
  };
}

// Sistema de decis√£o autom√°tica de AI Provider
interface MovieContext {
  genres?: string[];
  keywords?: string[];
  analysisLens?: number;
  isComplexDrama?: boolean;
}

export function selectOptimalAIProvider(context: MovieContext): AIProvider {
  const { genres = [], keywords = [], analysisLens, isComplexDrama } = context;

  // Converter para lowercase para compara√ß√£o
  const lowerGenres = genres.map(g => g.toLowerCase());
  const lowerKeywords = keywords.map(k => k.toLowerCase());

  // OpenAI necess√°rio para casos complexos
  const complexIndicators = [
    'coming-of-age', 'chegando √† maioridade', 'adolescente', 'autodescoberta',
    'thriller psicol√≥gico', 'suspense psicol√≥gico', 'psicol√≥gico',
    'drama complexo', 'trauma', 'depress√£o', 'sa√∫de mental'
  ];

  const isComplex = complexIndicators.some(indicator =>
    lowerKeywords.includes(indicator) || lowerGenres.includes(indicator)
  );

  // Coming-of-age sempre OpenAI
  if (isComplex || isComplexDrama) {
    return 'openai';
  }

  // Gemini excelente para estes g√™neros
  const geminiOptimalGenres = [
    'romance', 'com√©dia rom√¢ntica', 'fam√≠lia', 'anima√ß√£o',
    'com√©dia', 'aventura', 'a√ß√£o'
  ];

  const isGeminiOptimal = geminiOptimalGenres.some(genre =>
    lowerGenres.includes(genre) || lowerKeywords.includes(genre)
  );

  if (isGeminiOptimal) {
    return 'gemini';
  }

  // L√≥gica por lente de an√°lise
  switch (analysisLens) {
    case 13: // Feliz - Gemini bom para conte√∫do positivo
    case 17: // Animado - Gemini bom para energia
      return 'gemini';

    case 14: // Triste - Depende do contexto
      return isComplex ? 'openai' : 'gemini';

    case 16: // Ansioso - OpenAI melhor para suspense
      return 'openai';

    default:
      return 'gemini'; // Default para economia
  }
}

// Fun√ß√£o de conveni√™ncia para criar provider automaticamente
export function createAutoAIProvider(context: MovieContext): AIProviderManager {
  const provider = selectOptimalAIProvider(context);
  const config = getDefaultConfig(provider);
  return new AIProviderManager(config);
} 